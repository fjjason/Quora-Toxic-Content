{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pickle\nfrom gensim.models import Word2Vec\nimport gensim.models.keyedvectors as word2vec\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom matplotlib import pyplot\nfrom time import time\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.utils import np_utils\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.utils import shuffle\nfrom keras.callbacks import EarlyStopping\nfrom keras import*\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../\"))\nprint(os.listdir(\"../input\"))\nprint(os.listdir(\"../working\"))\nprint(os.listdir(\"../input/quora-insincere-questions-classification/embeddings/GoogleNews-vectors-negative300/\"))\n# Any results you write to the current directory are saved as output.",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['config', 'working', 'lib', 'input']\n['quora-insincere-questions-classification', 'vector']\n['.ipynb_checkpoints', 'submission.csv', '__notebook_source__.ipynb']\n['GoogleNews-vectors-negative300.bin']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_df = pd.read_csv(\"../input/quora-insincere-questions-classification/train.csv\")\ntest_df = pd.read_csv(\"../input/quora-insincere-questions-classification/test.csv\")\nprint(\"Train shape : \", train_df.shape)\nprint(\"Test shape : \", test_df.shape)",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train shape :  (1306122, 3)\nTest shape :  (375806, 2)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7eca2648a1ed26a937e1d416f1b02eef3b948d78"
      },
      "cell_type": "code",
      "source": "print(train_df.columns)\nprint(\"not toxic:\")\nprint(train_df[train_df.target==0][\"question_text\"].head())\nprint(\"toxic:\")     \nprint(train_df[train_df.target==1][\"question_text\"].head())",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Index(['qid', 'question_text', 'target'], dtype='object')\nnot toxic:\n0             How did Quebec nationalists see their province as a nation in the 1960s?\n1    Do you have an adopted dog, how would you encourage people to adopt and not shop?\n2                  Why does velocity affect time? Does velocity affect space geometry?\n3                            How did Otto von Guericke used the Magdeburg hemispheres?\n4        Can I convert montra helicon D to a mountain bike by just changing the tyres?\nName: question_text, dtype: object\ntoxic:\n22                           Has the United States become the largest dictatorship in the world?\n30        Which babies are more sweeter to their parents? Dark skin babies or light skin babies?\n110    If blacks support school choice and mandatory sentencing for criminals why don't they ...\n114    I am gay boy and I love my cousin (boy). He is sexy, but I dont know what to do. He is...\n115                                                         Which races have the smallest penis?\nName: question_text, dtype: object\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "96cd381d6ae93a8aa26bb73d086b0cf737662ac4",
        "_kg_hide-output": false,
        "_kg_hide-input": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "word2vec_model = word2vec.KeyedVectors.load_word2vec_format(\"../input/quora-insincere-questions-classification/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin\", binary=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "20fed7fc7b79e6a5ab364195f6e29ebc2e304f5a"
      },
      "cell_type": "markdown",
      "source": "**Average Feature Vector( some cleaned text):\n    Look through all words in the clean text\n    add the vector of each words into a new Numpy Array\n    divide by the number of words to get average\n    the returned value is the word2vec representation of the clean text**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2d127582465056b9da4eba5afb9b209c01926fb8"
      },
      "cell_type": "code",
      "source": "def avg_feature_vector(words):\n    #function to average all words vectors in a given paragraph\n    featureVec = np.zeros((300,), dtype=\"float32\")\n    nwords = 0\n    for word in words:\n        try:\n            nwords = nwords+1\n            featureVec = np.add(featureVec, np.array(word2vec_model.wv[word]))\n        except:\n            pass\n    if nwords>0:\n        featureVec = np.divide(featureVec, nwords)\n    return featureVec",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "befbba9f8e90f7131ddf7e5ac895f3e63bcb67bc"
      },
      "cell_type": "markdown",
      "source": "****Clean Text, takes very long so I loaded into a pickle file**************"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0937e4841c8477f7b26510bc9fc2582ebbdcc2f0",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "'''import nltk\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn\nstemmer = nltk.stem.SnowballStemmer('english')\nfrom nltk.stem import WordNetLemmatizer\nimport re\nfrom string import punctuation\nfrom sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\nstop_words = ENGLISH_STOP_WORDS\nimport numpy as np\nimport pandas as pd\nimport pickle\nlemmatizer = WordNetLemmatizer()\n\ndef preprocess(x):\n    x = x.encode(\"ascii\", errors=\"ignore\").decode()\n    x = x.lower().strip()\n    x = x.replace('\\r', ' ').replace('\\n', ' ')\n    x = re.sub(r'([\\w\\.-]+@[\\w\\.-]+)', 'email_se', x)\n    x = re.sub(' +',' ',x)\n    x = re.sub(r'[^\\w\\s]','',x)\n    x = nltk.word_tokenize(x) \n    x = [lemmatizer.lemmatize(word, pos='v') for word in x if lemmatizer.lemmatize(word, pos='v') not in stop_words]\n    x = [lemmatizer.lemmatize(word, pos='n') for word in x if lemmatizer.lemmatize(word, pos='n') not in stop_words]\n    x = [stemmer.stem(word) for word in x]\n    return x\ndef get_vector(text):\n    text = preprocess(text)\n    return avg_feature_vector(text)\ntrain_df[\"question_text_vector\"] = train_df[\"question_text\"].apply(get_vector)\nprint(train_df[\"question_text_vector\"])\ntrain_df.to_pickle(\"training_set_with_vectors\")'''",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "05a35c26d13eb8e5da6612c19010ca2ac76d9247"
      },
      "cell_type": "markdown",
      "source": "Read From pickle, input(word2vec representation) to X, output(labels(toxic,not toxic)) to Y "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5981da4043afd3a64b6bef9d5aec7ad0c0b4326a"
      },
      "cell_type": "code",
      "source": "train_df = pd.read_pickle(\"../input/vector/training_set_with_vectors\")\nprint(len(train_df))\nx = np.array(train_df.values)\nX = []\nfor i in range(len(x)):\n    b1 = x[i][3]\n    X.append(b1)\nY = np.array(train_df['target'].values)\nprint(len(Y))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "53d85afef5306b115b893c8f22cbf9b609083142"
      },
      "cell_type": "markdown",
      "source": "**One hot encoding for Y values**\n\n**Split data into testing/training set**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "af834d1beae863f83bb3241fb903a4f88bb3d162"
      },
      "cell_type": "code",
      "source": "encoder = LabelEncoder()\nencoder.fit(Y)\nY = encoder.transform(Y)\nY = np_utils.to_categorical(Y)\nX_train, X_test, y_train, y_test = train_test_split(np.array(X), Y, test_size=0.08, random_state=42)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9501c7f3bd717cb03bed55c216ebe4dc77c4646c"
      },
      "cell_type": "markdown",
      "source": "**Grid Search, Find the most optimal hyperparameters**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6b68bf3a8031b69620e746974ca5415ef1616c00"
      },
      "cell_type": "code",
      "source": "'''from sklearn.utils import class_weight\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.optimizers import * \nfrom keras.layers import Activation, Dense\n\ndef create_model(layers,activation,optimizer):\n    model = Sequential()\n    for i, nodes in enumerate(layers):\n        if i == 0:\n            model.add(Dense(nodes, input_dim=X_train.shape[1]))\n            model.add(Activation(activation))\n        else:\n            model.add(Dense(nodes))\n            model.add(Activation(activation))\n    model.add(Dense(2, activation = 'sigmoid'))\n    # Compile model\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    return model\nmodel = KerasRegressor(build_fn=create_model, verbose=0)\nlayers = [[200,100],[225,150,75],[220,160,100,50]]\nactivations = ['relu']\noptimizers =['adadelta','adam','rmsprop']\nparam_grid = dict(layers=layers,activation=activations, optimizer= optimizers, batch_size=[1024],epochs=[20,40])\ngrid = GridSearchCV(estimator=model,param_grid=param_grid,scoring='neg_mean_squared_error')\ngrid_result = grid.fit(X_train,y_train)'''",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8f20fc39a62b1cda1623146d156056f7da7956ce"
      },
      "cell_type": "code",
      "source": "'''[grid_result.best_score_,grid_result.best_params_]'''",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "541322a788867487dd7886969bfadd5629c34547"
      },
      "cell_type": "markdown",
      "source": "**Run the Model again with the most optimal hyperparameters found above**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "73281bfeabe6e4c0204d232f841171ea67d87ea6"
      },
      "cell_type": "code",
      "source": "model = Sequential()\ninput_dim = len(X_train[0])\nprint(input_dim)\n\n\nmodel.add(Dense(input_dim, input_dim = input_dim , activation = 'relu'))\nmodel.add(Dense(225, input_dim=X_train.shape[1]))\nmodel.add(Dense(150, input_dim=X_train.shape[1]))\nmodel.add(Dense(75, input_dim=X_train.shape[1]))\nmodel.add(Dense(2, activation = 'sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=['accuracy'])\nhistory = model.fit(X_train, y_train,\n        epochs=20,\n        batch_size=1024,\n        validation_split=0.2)\n#class_weight=class_weights\n\nprint()\ny_prediction = model.predict_classes(X_train)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9293bafc2c2dc624cfd535937138bb2a08417492"
      },
      "cell_type": "markdown",
      "source": "**Validation loss is constant while training loss decreased. **\n\n**Not the best result but not bad and there is no overffiting and no need for early stop or dropout****"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a83b387fccc671d25c289a553a77b0d51a64677e"
      },
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nplt.figure(figsize=(10,7))\na, = plt.plot(history.history['loss'],label='Training Loss')\nb, = plt.plot(history.history['val_loss'],label='Validation Loss')\nplt.legend(handles=[a,b])\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5d1b0f288e9f19e86cd08099d6fdea08dccfb8dc"
      },
      "cell_type": "markdown",
      "source": "Do the same preprocessing "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "288e21c7ea9f97962f75ed3f1666a7d79453618f"
      },
      "cell_type": "code",
      "source": "import nltk\nstemmer = nltk.stem.SnowballStemmer('english')\nfrom nltk.stem import WordNetLemmatizer\nimport re\nfrom string import punctuation\nfrom sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\nstop_words = ENGLISH_STOP_WORDS\nimport numpy as np\nimport pandas as pd\nimport pickle\nlemmatizer = WordNetLemmatizer()\n\ndef preprocess(x):\n    x = x.encode(\"ascii\", errors=\"ignore\").decode()\n    x = x.lower().strip()\n    x = x.replace('\\r', ' ').replace('\\n', ' ')\n    x = re.sub(r'([\\w\\.-]+@[\\w\\.-]+)', 'email_se', x)\n    x = re.sub(' +',' ',x)\n    x = re.sub(r'[^\\w\\s]','',x)\n    x = nltk.word_tokenize(x) \n    x = [lemmatizer.lemmatize(word, pos='v') for word in x if lemmatizer.lemmatize(word, pos='v') not in stop_words]\n    x = [lemmatizer.lemmatize(word, pos='n') for word in x if lemmatizer.lemmatize(word, pos='n') not in stop_words]\n    x = [stemmer.stem(word) for word in x]\n    return x\ndef get_vector(text):\n    text = preprocess(text)\n    return avg_feature_vector(text)\ntest_df[\"question_text_vector\"] = test_df[\"question_text\"].apply(get_vector)\nprint(test_df.head(1))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "20084a7bfad707f7711050d1c0e49425a47497a3"
      },
      "cell_type": "code",
      "source": "test1 = np.array(test_df.values)\n\ntest2 = []\nfor i in range(len(test1)):\n    b1 = test1[i][2]\n    test2.append(b1)\nprint(np.array(test2).shape)\npred_list = model.predict_classes(np.array(test2))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0fa6351808a22508340bd9535ba76968eb8e2a73"
      },
      "cell_type": "markdown",
      "source": "**Lets take a look at the model's performance on the testing set. **\n\nAll the 1's look pretty toxic to me. Have to say the model is performing fairly well.\nDon't know the exact accuracy since I missed the deadline for submission :("
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a07840d013b872e714aa0ae0ab7646cd7b4ba12e"
      },
      "cell_type": "code",
      "source": "pd.options.display.max_colwidth = 90\nsub = pd.DataFrame(\n    {'question': test_df[\"question_text\"].tolist() ,\n     'prediction': pred_list\n    })\nprint(\"toxic stuff\")\nprint(\"\")\nprint(\"\")\nprint(sub[sub.prediction==1].head(20))\nprint(\"\")\nprint(\"non-toxic stuff\")\nprint(\"\")\nprint(\"\")\nprint(sub[sub.prediction==0].head(20))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4ffb1ccb4c71f09dce2f39fc5decfc74b3d51ecc"
      },
      "cell_type": "code",
      "source": "sub = pd.DataFrame(\n    {'qid': test_df[\"qid\"].tolist() ,\n     'prediction': pred_list\n    })\nprint(sub.columns)\nprint(sub.head(4))\nprint(sub[sub.prediction==1].head(3))\nsub.to_csv(\"submission.csv\",index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6f12f377312bdd6b80f2ce4aeda3aed7c08af1ea"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "152e434ab1a044f4eb88f2b52503e81a91b83cbd"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}